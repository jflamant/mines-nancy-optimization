---
kernelspec:
    name: python3
---
# Linear conjugate gradient method

In this section, we will consider the following square system of $n$ equations
:::{math}
:label:eq:linear_problem_cg
Q x = p, \quad Q \succ 0
:::
where $x \in \mathbb{R}^n$, $Q \in \mathbb{R}^{n\times n}$ and $p\in \mathbb{R}^n$.

First, we recall that [](#eq:linear_problem_cg) is closely related to the study of least squares problems.

::::{hint} Reminder: least squares normal equations
We already encountered systems of the form [](#eq:linear_problem_cg) in the study of [least squares problems](../chapter3/01-intro.md).
Consider the least squares problem
:::{math}
\begin{array}{ll}
\minimize&  \Vert y - A x\Vert_2^2\\
\st & x \inÂ \mathbb{R}^n
\end{array}\quad \quad A \in \mathbb{R}^{m\times n}, y \in \mathbb{R}^m
:::
where we assume that $\rank{A} = n$. The *normal equations* for this optimization problem are
:::{math}
  \nabla f(x) = 0 \Leftrightarrow A^\top A x = A^\top y
:::
Now, pose $Q = A^\top A$ and $p = A^\top y$, we obtain the linear system [](#eq:linear_problem_cg). (Recall that $Q \succ 0$ since $\rank A = n$).
Hence the two problems are equivalent!
::::

## Preliminaries
### Conjugacy property

Conjugate gradient methods rely on the *conjugacy* property.

:::{prf:definition} Conjugacy property
:label:def:conjugacy
  Let $Q \in \mathbb{R}^{n\times n}$ a positive definite matrix.
  A set of non-zero vectors $\lbrace d_0, d_1, \ldots, d_\ell\rbrace$ is said to be *conjugate* with respect to $Q$ if
  $$
  d_i^\top Qd_j = 0\text{ for all } i\neq j.
  $$
:::
Note that if $d_1, \ldots, d_\ell$ are conjugate with respect to $Q$, then they are linearly independent. (Prove it).

This property motivates a first algorithm, known as the conjugate direction method.


### Conjugate directions method

:::{prf:algorithm} Conjugate direction method
:label:alg:CD_method
Let $x^{(0)} \in \mathbb{R}^n$ a starting point and $\lbrace d_0, d_1, \ldots, d_\ell\rbrace$ a set of conjugate directions.

At iteration $k$, generate the new iterate as
$$
  x^{(k+1)} = x^{(k)}+\alpha_k d_k
$$
where $\alpha_k$ is the **optimal step size** for the quadratic function associated to the systems of equations $Qx = p$.
:::

**Computation of the optimal stepsize $\alpha_k$**

  Let $\phi(\alpha) = \frac{1}{2} t(\alpha)^\top Qt(\alpha) - p^\top t(\alpha)$
  where $t(\alpha) = x^{(k)}+\alpha d_k$.
  Hence $\alpha_k = \argmin_{\alpha} \phi(\alpha)$.

  It is a quadratic convex function of $\alpha$, so it suffices to cancel out the derivative to get $\alpha_k$.
  \begin{align*}
    \phi'(\alpha) &= d_k^\top Q\left[x^{(k)}+\alpha d_k\right] - p^\top d_k\\
    &= \alpha d_k^\top Qd_k + d_k^\top\left[Qx^{(k)}-p\right]
  \end{align*}
  Define $r_k = Qx^{(k)}-p$ the residual at iteration $k$.
  Then
  $$
  \alpha_k = -\frac{{d_k^\top r_k}}{d_k^\top Qd_k}
  $$

:::{prf:theorem} @nocedal2006numerical [Theorem 5.1]
:label:thm:CD
The sequence $\lbrace x^{(k+1)}\rbrace$ generated by
$$x^{(k+1)} = x^{(k)}+\alpha_k d_k, \text{ where } \alpha_k = -\frac{{d_k^\top r_k}}{d_k^\top Qd_k}$$
where the $\lbrace d_k\rbrace$ are conjugate directions converges to the solution $x^\star$ of the linear system [](eq:linear_problem_cg) in at most $n$ steps.
:::

:::{prf:proof}
:nonumber:
:class:dropdown
Let us consider $n$ conjugate directions. This implies that the directions are linearly independent and thus they must span $\mathbb{R}^n$. In particular, there exist $\sigma_0, \ldots, \sigma_{N-1} \in \mathbb{R}$ such that
$$
  x^\star - x^{(0)} = \sum_{i=0}^{n-1} \sigma_i d_i.
$$
 By using the conjugacy property, we can obtain the value of $\sigma_k$ as
  \begin{align*}
    d_k^\top Q\left[x^\star - x^{(0)}\right] &=  d_k^\top Q\left[\sum_{i=0}^{n-1} \sigma_i d_i\right]= \sigma_k d_k^\top Qd_k
  \end{align*}
  hence $\sigma_k = \frac{ d_k^\top Q\left[x^\star - x^{(0)}\right]}{d_k^\top Qd_k}$. Next we prove that $\alpha_k = \sigma_k$.

  By the recursion formula, we have for $k>0, x^{(k)} = x^{(0)} + \sum_{i=0}^{k-1}\alpha_i d_i$. Then by conjugacy
  \begin{align*}
    d_k^\top Q x^{(k)} &= d_k^\top Q x^{(0)} \Leftrightarrow d_k^\top Q\left[ x^{(k)} - x^{(0)}\right] = 0
  \end{align*}
  and as a result
  $$
  d_k^\top Q\left[x^\star - x^{(0)}\right] = d_k^\top Q\left[x^\star - x^{(k)}\right] = d_k^\top \left[p - Qx^{(k)}\right] = - d_k^\top r_k
  $$
  which permits to conclude that $\alpha_k = \sigma_k$.
:::

In addition, we have the following additional properties.
:::{prf:property} Properties of the conjugate direction method
:label:prop:cd_method
Let $\lbrace x^{(k)}\rbrace $ be generated by the conjugate directions method given in [](#alg:CD_method). Then, for any iteration $k$, one has
- *Residual recurrence property*: $r_{k} = r_{k-1} + \alpha_{k-1} Qd_{k-1}$
- *Orthogonality of the residuals*: $d_i^\top r_k = 0$ for $i=0, 1, \ldots, k-1$
- $x^{(k)}$ is the minimizer of $\phi(x) = \frac{1}{2}x^\top Qx - p^\top x$ over the set $\lbrace x \mid  x^{(0)} + \operatorname{Span}\lbrace d_0, d_1, \ldots, d_{k-1}\rbrace\rbrace$
:::
Proofs can be found in @nocedal2006numerical [p. 106], if interested.

## The linear conjugate gradient (CG) method

 The conjugate directions method of  [](#alg:CD_method) requires a set of conjugate directions $\lbrace d_k\rbrace$, which can be determined in advance using, e.g.,
 - the eigenvalue decomposition of $Q$;
 - a modification of the Gram-Schmidt process to produce a set of conjugate directions.
This is, however, too costly for large scale applications.

:::{important} Solution: (linear) conjugate gradient (CG) method
- directions $d_k$ are computed iteratively;
- each new direction $d_k$ use only $d_{k-1}$; it is automatically conjugated to all the previous directions $d_{0},d_{1}, \ldots, d_{k-1}$;
-  cheap computational cost and memory storage.
:::

### Main idea and preliminary CG version
Choose the direction $d_k$ as a linear combination of $-r_k$ (negative residual) and $d_{k-1}$ (previous direction)
$$
d_k = -r_k + \beta_k d_{k-1}$$
where $\beta_k$ is set to impose conjugation between $d_k$ and $d_{k-1}$:
$$
d_{k-1}^\top Q d_k = 0 \Rightarrow \beta_k = \frac{d_{k-1}^\top Qr_{k}}{d_{k-1}^\top Qd_{k-1}}
$$
This gives us a preliminary version of CG.
Starting from $x^{(0)}\in \mathbb{R}^n$ and $d_0 = - r_0$, iterate until convergence
:::{math}
:label:eq:preliminary_cg
\begin{align*}
  x^{(k+1)} &= x^{(k)}+\alpha_k d_k, \text{ where } \alpha_k = -\frac{{d_k^\top r_k}}{d_k^\top Qd_k}\\
  d_{k+1} &= -r_{k+1}+\beta_{k+1} d_k, \text{ where } \beta_{k+1} = \frac{d_{k}^\top Qr_{k+1}}{d_{k}^\top Qd_{k}}
\end{align*}
:::
This first version is practical for *studying properties* of CG. To do this, we need to introduce the notion of *Krylov subspace*, which is very useful in numerical linear algebra..

:::{prf:definition} Krylov subspace
Given a matrix $Q$ of size $n\times n$ and a vector $r$ of size $n$,  the Krylov subspace of degree $k$ for $r$ is defined as
$\mathcal{K}(r; k) = \operatorname{Span}\lbrace r, Qr, Q^2 r, \ldots, Q^k r \rbrace$
:::

:::{prf:theorem} @nocedal2006numerical [Theorem 5.3]
:label:thm:prel_CG
  Consider the $k$-th iteration of the preliminary CG method [](#eq:preliminary_cg). Then
  $$
    \begin{align}
      r_k^\top r_i = 0 \text{ for } i=0, 1, \ldots, k-1\\
      \operatorname{Span}\lbrace r_0, r_1, \dots, r_k\rbrace = \mathcal{K}(r_0;k)\\
      \operatorname{Span}\lbrace d_0, d_1, \dots, d_k\rbrace = \mathcal{K}(r_0;k)\\
      d_k^\top Qd_i = 0 \text{ for } i=0, 1, \ldots, k-1
    \end{align}
    $$
    and thus the sequence $\lbrace x^{(k)}\rbrace$ converges to $x^\star$ in at most $n$ iterations.
  :::
  :::{prf:proof}
  :nonumber:
  :class:dropdown
  See @nocedal2006numerical [pp. 109-111].
  :::
Here are some important takeaways from the theorem:
- residuals $\lbrace r_k\rbrace$ are mutually orthogonal;
- Residuals $r_k$ and search directions $d_k$ all belong to the Krylov subspace of order $k$ associated with $r_0$;
- the search directions $d_0, d_1, \ldots, d_{N-1}$ are conjugate wrt $Q$
- by the [conjugate direction theorem](#thm:CD), this implies termination in at most $n$ steps.
- these results all depend on the choice of $d_0$! In fact, if one chooses $d_0 \neq - r_0$ (i.e., different from the steepest direction at $x^{(0)}$) then the theorem does not longer holds.

### Practical CG algorithm

  Using [](#prop:cd_method) and [](#thm:prel_CG), we can **improve the computations of $\alpha_k$ and $\beta_{k+1}$ in CG**:
$$\begin{align*}
  \alpha_k = -\frac{d_k^\top r_k}{{d_k^\top Qd_k}} &= -\frac{\left[-r_k + \beta_kd_{k-1}\right]^\top r_k}{d_k^\top Qd_k} = \frac{\Vert r_k\Vert_2^2}{{d_k^\top Qd_k}}
  \end{align*}
$$
  Moreover, observe that $r_{k+1}-r_k = \alpha_k Qd_k$ so that
  $$
    \beta_{k+1} = \frac{d_{k}^\top Qr_{k+1}}{d_{k}^\top Qd_{k}} = \frac{{d_k^\top Qd_k}}{\Vert r_k\Vert_2^2} \frac{\left[r_{k+1}-r_k\right]^\top r_{k+1}}{d_k^\top Qd_k} = \frac{\Vert r_{k+1}\Vert_2^2}{\Vert r_{k}\Vert_2^2}
  $$
where we used the orthogonality of residuals.
This leads to the following practical algorithm.

:::{prf:algorithm} Standard linear CG algorithm
:label: alg:linear-cg
1. **Input:**  $x^{(0)}$
2. Set $r_0 = Qx^{(0)} - p$ and $d_0 = -r_0$
3. Set  $k = 0$
4. **While** $\|r_k\|_2 \neq 0$ **do**:
    - $\alpha_k = \|r_k\|_2^2 / (d_k^\top Q d_k) $
    - $ x^{(k+1)} = x^{(k)} + \alpha_k d_k $
    - $\beta_{k+1} = \|r_{k+1}\|_2^2 / \|r_k\|_2^2 $
    - $d_{k+1} = -r_{k+1} + \beta_{k+1} d_k $
    - $ k = k+1 $
5. **Return** $x^{(k)}$
:::

### Numerical illustration

[](#thm:prel_CG) tells us convergence of CG to $x^\star$ is guaranteed after at most $n$ iterations. However, it can be much faster!

:::{prf:example}
Solve system $Qx = p$ using CG when $Q=\lambda {I}_n, \lambda > 0$.

First residual $r_0 = \lambda x^{(0)} - p$ and direction $d_0 = -r_0$.
First iteration:
$$x^{(1)} = x^{(0)} - \frac{\Vert \lambda x^{(0)} - p\Vert^2}{\lambda \Vert  \lambda x^{(0)} - p\Vert^2} (\lambda x^{(0)} - p) =  x^{(0)} -   x^{(0)} + \lambda^{-1}p = x^\star$$
Then $r_{k+1} = 0$ and $d_{k+1} = 0$. CG stops after exactly 1 iteration, for any $x^{(0)}$!
:::

In fact,
- convergence of CG usually depends on the \alert{distribution of eigenvalues of $Q$}
- it can be shown that if $Q$ has $r$ distinct eigenvalues, then CG converges in at most $r$ iterations.

The following example shows how CG convergence speed can be affected by the distrubution of eigenvalues.

```{code-cell} python

import numpy as np
import matplotlib.pyplot as plt
from scipy.sparse.linalg import cg
from mpl_toolkits.axes_grid1.inset_locator import inset_axes

# Function to run CG on a system with matrix A and show convergence rates
def run_cg(A, b):
    x0 = np.zeros_like(b)
    residuals = []

    # Custom callback function to record residuals at each iteration
    def callback(xk):
        residuals.append(np.linalg.norm(b - A @ xk))

    # Run CG algorithm
    x, _ = cg(A, b, x0=x0, rtol=1e-14, callback=callback, maxiter=x0.size)
    return residuals

# Problem setup
n = 100
np.random.seed(10)
b = np.random.randn(n)

# Case 1: Well-clustered eigenvalues
eigenvalues_clustered = 10 + 10 * np.random.rand(n)
A_clustered = np.diag(eigenvalues_clustered)
residuals_clustered = run_cg(A_clustered, b)

# Case 2: Spread-out eigenvalues
eigenvalues_spread = 100 * np.random.rand(n)
A_spread = np.diag(eigenvalues_spread)
residuals_spread = run_cg(A_spread, b)

# Plot convergence rates
fig, ax = plt.subplots(figsize=(10, 6))
ax.semilogy(residuals_clustered, label="Clustered Eigenvalues")
ax.semilogy(residuals_spread, label="Spread Eigenvalues")
ax.set_xlabel("Iteration")
ax.set_ylabel("Residual Norm (log scale)")
ax.set_title("Convergence of Conjugate Gradient for Different Eigenvalue Distributions")
ax.grid(True)
ax.legend()

# Add inset for eigenvalue distributions
axins = inset_axes(ax, width="35%", height="35%", loc='upper right')
axins.hist(eigenvalues_clustered, bins=20, alpha=0.6, label="Clustered")
axins.hist(eigenvalues_spread, bins=20, alpha=0.6, label="Spread")
#axins.set_title("Eigenvalue Distribution", fontsize=10)
axins.set_xlabel("Eigenvalue", fontsize=8)
axins.set_ylabel("Count", fontsize=8)
axins.tick_params(axis='both', which='major', labelsize=8)
axins.legend(fontsize=8)

plt.show()

```

## Summary

:::{important} Standard linear conjugate gradient method
**For which problems?**
- Solve "square" linear systems of the form $Qx = p$ with $Q \succ 0$
- For strictly convex quadratic problems with objective $\frac{1}{2}x^\top Qx - p^\top x, Q \succ 0$ (or overdetermined - full column-rank - least-squares problems)

**Key properties**
- simple, low-memory algorithm; very efficient in practice
- convergence in at most $n$ steps
- convergence rate depend on the distribution of eigenvalues of $Q$: the closer the condition number of $Q$ is to $1$, the faster the convergence; use of preconditionners to improve the condition number
- in Python, implemented in [`scipy.sparse.linalg.cg`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.linalg.cg.html)
:::
