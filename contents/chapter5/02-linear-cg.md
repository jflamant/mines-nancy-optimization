# Linear conjugate gradient method

In this section, we will consider the following square system of $n$ equations
:::{math}
:label:eq:linear_problem_cg
Q x = p, \quad Q \succ 0
:::
where $x \in \mathbb{R}^n$, $Q \in \mathbb{R}^{n\times n}$ and $p\in \mathbb{R}^n$.

First, we recall that [](#eq:linear_problem_cg) is closely related to the study of least squares problems.

::::{hint} Reminder: least squares normal equations
We already encountered systems of the form [](#eq:linear_problem_cg) in the study of [least squares problems](../chapter3/01-intro.md).
Consider the least squares problem
:::{math}
\begin{array}{ll}
\minimize&  \Vert y - A x\Vert_2^2\\
\st & x \inÂ \mathbb{R}^n
\end{array}\quad \quad A \in \mathbb{R}^{m\times n}, y \in \mathbb{R}^m
:::
where we assume that $\rank{A} = n$. The *normal equations* for this optimization problem are
:::{math}
  \nabla f(x) = 0 \Leftrightarrow A^\top A x = A^\top y
:::
Now, pose $Q = A^\top A$ and $p = A^\top y$, we obtain the linear system [](#eq:linear_problem_cg). (Recall that $Q \succ 0$ since $\rank A = n$).
Hence the two problems are equivalent!
::::

## Preliminaries
### Conjugacy property

Conjugate gradient methods rely on the *conjugacy* property.

:::{prf:definition} Conjugacy property
:label:def:conjugacy
  Let $Q \in \mathbb{R}^{n\times n}$ a positive definite matrix.
  A set of non-zero vectors $\lbrace d_0, d_1, \ldots, d_\ell\rbrace$ is said to be *conjugate* with respect to $Q$ if
  $$
  d_i^\top Qd_j = 0\text{ for all } i\neq j.
  $$
:::
Note that if $d_1, \ldots, d_\ell$ are conjugate with respect to $Q$, then they are linearly independent. (Prove it).

This property motivates a first algorithm, known as the conjugate direction method.


### Conjugate directions method

:::{prf:algorithm} Conjugate direction method
:label:alg:CD_method
Let $x^{(0)} \in \mathbb{R}^n$ a starting point and $\lbrace d_0, d_1, \ldots, d_\ell\rbrace$ a set of conjugate directions.

At iteration $k$, generate the new iterate as
$$
  x^{(k+1)} = x^{(k)}+\alpha_k d_k
$$
where $\alpha_k$ is the **optimal step size** for the quadratic function associated to the systems of equations $Qx = p$.
:::

**Computation of the optimal stepsize $\alpha_k$**

  Let $\phi(\alpha) = \frac{1}{2} t(\alpha)^\top Qt(\alpha) - p^\top t(\alpha)$
  where $t(\alpha) = x^{(k)}+\alpha d_k$.
  Hence $\alpha_k = \argmin_{\alpha} \phi(\alpha)$.

  It is a quadratic convex function of $\alpha$, so it suffices to cancel out the derivative to get $\alpha_k$.
  \begin{align*}
    \phi'(\alpha) &= d_k^\top Q\left[x^{(k)}+\alpha d_k\right] - p^\top d_k\\
    &= \alpha d_k^\top Qd_k + d_k^\top\left[Qx^{(k)}-p\right]
  \end{align*}
  Define $r_k = Qx^{(k)}-p$ the residual at iteration $k$.
  Then
  $$
  \alpha_k = -\frac{{d_k^\top r_k}}{d_k^\top Qd_k}
  $$

:::{prf:theorem} @nocedal2006numerical [Theorem 5.1]
The sequence $\lbrace x^{(k+1)}\rbrace$ generated by
$$x^{(k+1)} = x^{(k)}+\alpha_k d_k, \text{ where } \alpha_k = -\frac{{d_k^\top r_k}}{d_k^\top Qd_k}$$
where the $\lbrace d_k\rbrace$ are conjugate directions converges to the solution $x^\star$ of the linear system [](eq:linear_problem_cg) in at most $n$ steps.
:::

:::{prf:proof}
:nonumber:
:class:dropdown
Let us consider $n$ conjugate directions. This implies that the directions are linearly independent and thus they must span $\mathbb{R}^n$. In particular, there exist $\sigma_0, \ldots, \sigma_{N-1} \in \mathbb{R}$ such that
$$
  x^\star - x^{(0)} = \sum_{i=0}^{N-1} \sigma_i d_i.
$$
 By using the conjugacy property, we can obtain the value of $\sigma_k$ as
  \begin{align*}
    d_k^\top Q\left[x^\star - x^{(0)}\right] &=  d_k^\top Q\left[\sum_{i=0}^{N-1} \sigma_i d_i\right]= \sigma_k d_k^\top Qd_k
  \end{align*}
  hence $\sigma_k = \frac{ d_k^\top Q\left[x^\star - x^{(0)}\right]}{d_k^\top Qd_k}$. Next we prove that $\alpha_k = \sigma_k$.

  By the recursion formula, we have for $k>0, x^{(k)} = x^{(0)} + \sum_{i=0}^{k-1}\alpha_i d_i$. Then by conjugacy
  \begin{align*}
    d_k^\top Q x^{(k)} &= d_k^\top Q x^{(0)} \Leftrightarrow d_k^\top Q\left[ x^{(k)} - x^{(0)}\right] = 0
  \end{align*}
  and as a result
  $$
  d_k^\top Q\left[x^\star - x^{(0)}\right] = d_k^\top Q\left[x^\star - x^{(k)}\right] = d_k^\top \left[p - Qx^{(k)}\right] = - d_k^\top r_k
  $$
  which permits to conclude that $\alpha_k = \sigma_k$.
:::

In addition, we have the following additional properties.
:::{prf:property} Properties of the conjugate direction method

Let $\lbrace x^{(k)}\rbrace $ be generated by the conjugate direction method given in [](#alg:CD_method). Then, for any iteration $k$, one has
- *Residual recurrence property*: $r_{k} = r_{k-1} + \alpha_{k-1} Qd_{k-1}$
- *Orthogonality of the residuals*: $d_i^\top r_k = 0$ for $i=0, 1, \ldots, k-1$
- $x^{(k)}$ is the minimizer of $\phi(x) = \frac{1}{2}x^\top Qx - p^\top x$ over the set $\lbrace x \mid  x^{(0)} + \operatorname{Span}\lbrace d_0, d_1, \ldots, d_{k-1}\rbrace\rbrace$
:::
Proofs can be found in @nocedal2006numerical [p. 106], if interested.

## The linear conjugate gradient (CG) method


### Properties

### Additional comments

### Algorithm


### Numerical illustration

## Summary
