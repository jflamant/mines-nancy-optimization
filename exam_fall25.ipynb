{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jflamant/mines-nancy-optimization/blob/main/exam_fall25.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q579TxJkcbWA"
      },
      "source": [
        "# Mines Nancy - Fall 2025 - Numerical Optimization\n",
        "----\n",
        "## Exam: unconstrained and constrained optimization algorithms\n",
        "\n",
        "The objective of this exam is to demonstrate your ability to implement several optimization algorithms, display results and comment your work.\n",
        "\n",
        "### Running the Notebook\n",
        "This notebook can be executed in the following environments:\n",
        "- **Google Colab**: A convenient, cloud-based option that requires no setup. Simply open the notebook in Colab, and you're ready to run the code.\n",
        "- **Locally**: You can run the notebook on your local machine using environments like **JupyterLab** or **Jupyter Notebook**. You can also run it directly in **Visual Studio Code** if you have the Jupyter extension. In all cases, ensure you have Python installed along with the required libraries, `NumPy` and `Matplotlib`, ``scipy``.\n",
        "\n",
        "\n",
        "### Instructions: read carefully before starting.\n",
        "- **duration**: 1 hour 45 min\n",
        "- **modalities of grading**: code quality, comments, and results.\n",
        "- **deliverable**: this .ipynb file, completed with your answers. To be uploaded on Arche [link](https://arche.univ-lorraine.fr/course/view.php?id=74098) at the end of the session.\n",
        "- **make sure your code can be executed in a linear way**. In particular, check the code is **free from errors when you run execute all**.\n",
        "- **documents**: you have access to all code developed during the course, lecture slides on Arche, and the course website. You can access the documentation of Python packages on the internet if you need to.\n",
        "- The exam is probably too long. Each exercise is independent. Focus on what you know to do, and do your best.\n",
        "\n",
        "\n",
        "> ### use of AI tools (Github Copilot, ChatGPT, Gemini, etc) is prohibited (and will be detected). Failure to do so is a fraud, and will result in a strong penalty.\n",
        "\n",
        "Good luck!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XwRwdnVbcbWC"
      },
      "outputs": [],
      "source": [
        "# all imports for the exam\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gefN2AW7cbWD"
      },
      "source": [
        "# Exercise 1: first and second-order algorithms\n",
        "\n",
        "Consider the following unconstrained optimization problem\n",
        "$$ \\begin{array}{ll}\n",
        "\\operatorname{minimize}&\\quad f_0(x):= \\log(e^{x_1} + e^{x_2}) + \\frac{1}{2}(x_1^2 + x_2^2)\\\\\n",
        "\\operatorname{subject\\: to}&\\quad x \\in \\mathbb{R}^2\n",
        "\\end{array}$$\n",
        "\n",
        "This problem is convex and a minimum exists. We will study algorithms to solve this problem.\n",
        "\n",
        "Recall from the theoretical part of the exam\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "\\nabla f_0(x) &= \\begin{bmatrix}\n",
        "            \\dfrac{e^{x_1}}{e^{x_1} +e^{x_2}} + x_1\\\\\n",
        "             \\dfrac{e^{x_2}}{e^{x_1} +e^{x_2}} + x_2\n",
        "        \\end{bmatrix}\\\\\n",
        "\\nabla^2 f_0(x) &= \\frac{1}{(e^{x_1}+e^{x_2})^2}\\begin{bmatrix}\n",
        "            e^{x_1}+e^{x_2} + e^{2x_2} + 2e^{x_1+x_2} & -e^{x_1+x_2}\\\\\n",
        "            -e^{x_1 + x_2} & e^{x_1}+e^{x_2} + e^{2x_1} + 2e^{x_1+x_2}\n",
        "        \\end{bmatrix}\n",
        "\\end{align*}\n",
        "$$\n",
        "\n",
        "**Questions**\n",
        "1. Construct a function ``objective(x1,x2)`` which computes the value of $f(x)$ for any $x=(x_1, x_2) \\in \\mathbb{R}^2$.\n",
        "2. Display the objective function on a grid such that $(x_1, x_2) \\in [-2, 1] \\times [-2, 1]$. Take $N=100$ points along each axis. Use your preferred vizualization, but don't forget to label axes!\n",
        "3. Program a function ``gradf(x)`` which returns the gradient of $f$ at a point ${x} = [x_1, x_2]^\\top$. Similarly, program a function ``hessianf(x)`` which returns the Hessian of $f$ at a point $x = [x_1, x_2]^\\top$.\n",
        "4. verify numerically that $x^* = [-1/2, -1/2]^\\top$ is a global minimum for this optimization problem.\n",
        "5. **Gradient descent with constant stepsize**.\n",
        "   1. Solve the problem using gradient descent *with constant step size* from a random initial point in $[-2, -1]^2$. in particular, the algorithm must include\n",
        "      - a stopping criterion based on the improvement of the cost function and a maximum number of iterations;\n",
        "      - the recording of successive iterates ${x}^{(k)}$ and cost values $f_0({x}^{(k)})$;\n",
        "   2. For four different values of stepsize, display your results in two ways:\n",
        "      -  represent the iterates on top of the 2D plot of objective function\n",
        "      -  represent the quantity $f_0({x}^{(k)}) - f_0({x}^\\star)$ vs iterations\n",
        "      -  compute the distance between the last iterate and the global minimizer for each stepsize.\n",
        "   3. Using try and error, fix ${x}^{(0)} = [-2, -1]^\\top$ and determine empirically (up to one decimal) the maximum stepsize one can use for this problem (illustrate!)\n",
        "\n",
        "7. **Newton method**. Implement the Newton method (with unit stepsize) for this problem. Starting from an arbitrary point, display your results:\n",
        "      -  represent the iterates on top of the 2D plot of objective function\n",
        "      -  represent the quantity $f_0({x}^{(k)}) - f_0({x}^\\star)$ vs iterations\n",
        "      -  compute the distance between the last iterate and the global minimizer for each stepsize."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WZc7p_i8cbWE"
      },
      "outputs": [],
      "source": [
        "# question 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qGb-9C5hcbWF"
      },
      "outputs": [],
      "source": [
        "# question 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UbyJ8tDMcbWF"
      },
      "outputs": [],
      "source": [
        "# question 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CGJNuQbjcbWF"
      },
      "outputs": [],
      "source": [
        "# question 4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uxV5dUzUcbWG"
      },
      "outputs": [],
      "source": [
        "# question 5.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "At_9K1CTcbWG"
      },
      "outputs": [],
      "source": [
        "# question 5.2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0YefsAq_cbWH"
      },
      "outputs": [],
      "source": [
        "# question 5.3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "beuZOzQgcbWH"
      },
      "outputs": [],
      "source": [
        "# question 6:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L9ugs2JbcbWH"
      },
      "source": [
        "# Exercise 2: constrained optimization\n",
        "\n",
        "Consider the following optimization problem\n",
        "$$\\begin{array}{ll}\n",
        "   \\operatorname{minimize}&\\quad -\\sum_{i=1}^n k_i \\log x_i + \\frac{\\rho}{2} \\Vert x\\Vert_2^2\\\\\n",
        "    \\operatorname{subject\\: to} &\\quad x \\in \\mathcal{S}\n",
        "\\end{array}$$\n",
        "where $k_1, \\ldots, k_n > 0$ are known, $\\rho \\geq 0$ is an hyperparameter, and where $\\mathcal{S}$ denotes the \\emph{simplex} defined as\n",
        "$$\n",
        "\\mathcal{S} := \\left\\lbrace x\\in \\mathbb{R}^n\\mid \\sum_{i=1}^n x_i  = 1\\text{ and } x_i \\geq 0, i=1, \\ldots n\\right\\rbrace$$\n",
        "\n",
        "We recall that for $\\rho = 0$, the unique solution to this problem is given by\n",
        "\n",
        "$$ x_i^\\star = \\frac{k_i}{\\sum_{i=1}^n k_i}, \\quad i=1, \\ldots, n$$\n",
        "\n",
        "The goal of this exercise is to solve the problem numerically when $\\rho > 0$.\n",
        "\n",
        "We provide below the utility function ``proj_simplex`` to project a vector $v\\in \\mathbb{R}^n$ onto the simplex $\\mathcal{S}$, i.e., it solves\n",
        "$$P_{\\mathcal{S}}(v) \\in \\argmin_{x \\in \\mathcal{S}} \\Vert x - v\\Vert_2^2$$\n",
        "\n",
        "1. Execute the code below and construct a function `costf(x, ks=ks, rho=rho)` that evaluates the objective values. Plot the optimal solution of the problem for $\\rho = 0$. Print the corresponding value of the objective.\n",
        "2. **Projected gradient descent**\n",
        "    1. Write a function `gradf(x, ks=ks, rho=rho)` that returns the gradient of the objective. Don't forget to define the variable rho outside the function!\n",
        "    2. Write a function `lipschitz_constant(ks=ks, rho=rho)` that returns the *approximate** lipschitz constant of the problem given by\n",
        "    $$\\lambda_{\\max}(\\nabla^2 f_0(x)\\vert_{x_i = 1/n}) + \\rho$$\n",
        "    i.e., it is the sum of $\\rho$ and the largest eigenvalue of the Hessian of $f_0$ evaluated at $x_i = 1/n, i=1, \\ldots, n$ when $\\rho=0$.\n",
        "\n",
        "    3. Using the course material, write a function that performs projected gradient descent with constant stepsize $1/L_{\\rho}$, where $L_\\rho$ is determined by `lipschitz_constant`. The function should incorporate\n",
        "        - a stopping criterion based on the improvement of the cost function and a maximum number of iterations;\n",
        "        - the recording of successive iterates ${x}^{(k)}$ and cost values $f_0({x}^{(k)})$;\n",
        "    4. Verify that the algorithm converges fo  $\\rho = 10^4$ and plot the evolution of the objective function. Use $x^{(0)}$ as the solution obtained for $\\rho = 0$ as initial point.\n",
        "    5. Compare solutions $x^\\star$ for $\\rho = [10^4, 10^5, 10^6]$. What happens in the limit $\\rho \\to \\infty?$.\n",
        "3. (**Bonus**) Uzawa's method. Considering that inequality constraints are inactive at feasible points, the problem can be simplified as the equality constrained problem\n",
        "$$\\begin{array}{ll}\n",
        "   \\operatorname{minimize}&\\quad -\\sum_{i=1}^n k_i \\log x_i + \\frac{\\rho}{2} \\Vert x\\Vert_2^2\\\\\n",
        "    \\operatorname{subject\\: to} &\\quad \\sum_{i=1}^n x_i = 1\n",
        "\\end{array}$$\n",
        "with Lagrangian $L(x, \\nu) = -\\sum_{i=1}^n k_i \\log x_i + \\frac{\\rho}{2} \\Vert x\\Vert_2^2 + \\nu(\\sum_{i=1}^n x_i -1)$.\n",
        "For fixed $\\nu$, the Lagrangian is minimized in $x$ by solving the quadratic equation\n",
        "$$\\rho x_i^2 + \\nu x_i - k_i = 0, \\quad i=1, \\ldots, n$$\n",
        "and selecting $x_i > 0$.\n",
        "\n",
        "    1.  For fixed $\\nu$, write a function `solve_lagrangian_fixed_nu(nu, ks=ks, rho=rho)` that returns $x$ minimizing the Lagrangian. Use `np.roots` to solve the quadratic equation\n",
        "    2. Implement Uzawa's method (with fixed step size $\\alpha = 100$.) and determine $\\nu^\\star$ for $\\rho = [10^4, 10^5, 10^6]$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GQd-wc90cbWI"
      },
      "outputs": [],
      "source": [
        "#  to be executed before starting the exercise\n",
        "\n",
        "def get_data_for_ex2():\n",
        "\n",
        "    np.random.seed(2026)\n",
        "    counts = np.random.rand(100) # k values\n",
        "\n",
        "    ks = (np.convolve(counts, np.hanning(9), \"same\"))\n",
        "    return ks\n",
        "\n",
        "def proj_simplex(v, z=1):\n",
        "    n_features = v.shape[0]\n",
        "    u = np.sort(v)[::-1]\n",
        "    cssv = np.cumsum(u) - z\n",
        "    ind = np.arange(n_features) + 1\n",
        "    cond = u - cssv / ind > 0\n",
        "    rho = ind[cond][-1]\n",
        "    theta = cssv[cond][-1] / float(rho)\n",
        "    w = np.maximum(v - theta, 1e-12)\n",
        "    return w\n",
        "\n",
        "ks = get_data_for_ex2() # get list of k values k_1 = ks[0], k_2 = ks[1], etc.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QRpGcl5rcbWI"
      },
      "outputs": [],
      "source": [
        "# question 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZjM4M0OucbWJ"
      },
      "outputs": [],
      "source": [
        "# question 2.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eyGQx2zgcbWJ"
      },
      "outputs": [],
      "source": [
        "# question 2.2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OX6Xrzh5cbWJ"
      },
      "outputs": [],
      "source": [
        "# question 2.3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z4LPpIM1cbWJ"
      },
      "outputs": [],
      "source": [
        "# question 2.4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7plfs8_WcbWJ"
      },
      "outputs": [],
      "source": [
        "# question 2.5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FNQUEYddcbWK"
      },
      "outputs": [],
      "source": [
        "# question 3.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9AIdICmJcbWK"
      },
      "outputs": [],
      "source": [
        "# question 3.2\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vz0jNTjScbWK"
      },
      "source": [
        "# Bonus exercise: a large least squares problem\n",
        "\n",
        "We would like to solve the least squares problem\n",
        "\n",
        "$$\n",
        "\\begin{array}{ll}\n",
        "\\operatorname{minimize}&\\quad \\Vert {b} - {A}_i{x}\\Vert_2^2\\\\\n",
        "\\operatorname{subject\\: to}&\\quad x \\in \\mathbb{R}^n\n",
        "\\end{array},\\qquad i =1, 2$$\n",
        "for two large matrices ${A}_1$ and ${A}_2$ in $\\mathbb{R}^{n\\times n}$ with $n=1024$ and the same vector ${b}$.\n",
        "\n",
        "**Questions**\n",
        "1. Solve this problem using linear conjugate gradient (CG) applied to the normal equations of the LS problem.\n",
        "   1. Implement the linear conjugate gradient algorithm by following the [course material](https://jflamant.github.io/mines-nancy-optimization/linear-cg/) (Algorithm 2).\n",
        "   2. Monitor and display the norm of residual ${r}_k = \\Vert {A}{x}^{(k)} -{b}\\Vert_2^2$ throughout iterations\n",
        "   3. display the convergence (in terms of the norm of the residual) for the two matrices. How do you explain the difference?\n",
        "2. compare the solutions obtained by conjugate gradient with that obtained by the usual least-square explicit solution, in terms of:\n",
        "   1. residual error\n",
        "   2. computation time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fu1EKP7DcbWK"
      },
      "outputs": [],
      "source": [
        "# to be executed before starting the exercise\n",
        "\n",
        "def get_data_for_exo3():\n",
        "    np.random.seed(2026)\n",
        "    N = 1024\n",
        "    b = np.random.randn(N)\n",
        "\n",
        "    # def of A1, A2\n",
        "    temp = np.random.randn(N, N)\n",
        "    u, s, vh = np.linalg.svd(temp)\n",
        "    d1 = np.diag(1+0.1*np.random.rand(N))\n",
        "    d2 = np.diag(0.1+1*np.random.rand(N))\n",
        "\n",
        "    A1 = u @ d1 @ vh\n",
        "    A2 = u @ d2 @ vh\n",
        "\n",
        "    return A1, A2, b\n",
        "\n",
        "A1, A2, b = get_data_for_exo3()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xVhP7cRCcbWK"
      },
      "outputs": [],
      "source": [
        "# question 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bd9Kq5rdcbWK"
      },
      "outputs": [],
      "source": [
        "# question 2"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "3.11.1",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.1"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}